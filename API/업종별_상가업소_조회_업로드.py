# Databricks notebook source
# MAGIC %md
# MAGIC # 1. ì—…ë¡œë“œ 

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType
from pyspark.sql.functions import row_number
from pyspark.sql.window import Window
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import time

# âœ… PySpark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("API Data Collection for ì—…ì¢…ë³„ ìƒê°€ì—…ì†Œ ì¡°íšŒ") \
    .config("spark.databricks.delta.schema.autoMerge.enabled", "true") \
    .getOrCreate()

# âœ… API URL ë° Delta í…Œì´ë¸”
url = "https://apis.data.go.kr/B553077/api/open/sdsc2/storeListInUpjong"
catalog_table = "bronze.api_public.`ì—…ì¢…ë³„_ìƒê°€ì—…ì†Œ_ì¡°íšŒ`"

# âœ… ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("ID", LongType(), True),
    StructField("bizesId", StringType(), True),
    StructField("bizesNm", StringType(), True),
    StructField("brchNm", StringType(), True),
    StructField("indsLclsCd", StringType(), True),
    StructField("indsLclsNm", StringType(), True),
    StructField("indsMclsCd", StringType(), True),
    StructField("indsMclsNm", StringType(), True),
    StructField("indsSclsCd", StringType(), True),
    StructField("indsSclsNm", StringType(), True),
    StructField("ksicCd", StringType(), True),
    StructField("ksicNm", StringType(), True),
    StructField("ctprvnCd", StringType(), True),
    StructField("ctprvnNm", StringType(), True),
    StructField("signguCd", StringType(), True),
    StructField("signguNm", StringType(), True),
    StructField("adongCd", StringType(), True),
    StructField("adongNm", StringType(), True),
    StructField("ldongCd", StringType(), True),
    StructField("ldongNm", StringType(), True),
    StructField("lnoCd", StringType(), True),
    StructField("plotSctCd", StringType(), True),
    StructField("plotSctNm", StringType(), True),
    StructField("lnoMnno", StringType(), True),
    StructField("lnoSlno", StringType(), True),
    StructField("lnoAdr", StringType(), True),
    StructField("rdnmCd", StringType(), True),
    StructField("rdnm", StringType(), True),
    StructField("bldMnno", StringType(), True),
    StructField("bldSlno", StringType(), True),
    StructField("bldMngNo", StringType(), True),
    StructField("bldMngNm", StringType(), True),
    StructField("rdnmAdr", StringType(), True),
    StructField("oldZipcd", StringType(), True),
    StructField("newZipcd", StringType(), True),
    StructField("dongNo", StringType(), True),
    StructField("flrNo", StringType(), True),
    StructField("hoNo", StringType(), True),
    StructField("lon", StringType(), True),
    StructField("lat", StringType(), True),
    StructField("x", DoubleType(), True),
    StructField("y", DoubleType(), True),
    StructField("collected_time", StringType(), True)
])

# âœ… Delta í…Œì´ë¸” ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_delta_table():
    try:
        spark.table(catalog_table)
        print(f"Delta í…Œì´ë¸” '{catalog_table}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    except Exception:
        print(f"Delta í…Œì´ë¸” '{catalog_table}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        empty_df = spark.createDataFrame([], schema)
        empty_df.write.format("delta").mode("overwrite").saveAsTable(catalog_table)
        print(f"Delta í…Œì´ë¸” '{catalog_table}' ìƒì„± ì™„ë£Œ.")

# âœ… ì•ˆì „í•œ float ë³€í™˜
def safe_float(value):
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

# âœ… í˜ì´ì§€ë³„ ì €ì¥ í•¨ìˆ˜
def save_to_delta(data):
    if data:
        for item in data:
            item["x"] = safe_float(item.get("x"))
            item["y"] = safe_float(item.get("y"))

        df = spark.createDataFrame(data, schema=schema)

        # ID ì»¬ëŸ¼ ì œê±° í›„ ì¬ìƒì„±
        if "ID" in df.columns:
            df = df.drop("ID")

        window_spec = Window.orderBy("bizesId")
        df = df.withColumn("ID", row_number().over(window_spec).cast(LongType()))

        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬: IDê°€ ë§¨ ì•
        columns = df.columns
        columns.remove("ID")
        df = df.select(["ID"] + columns)

        # ì „ì²´ ì¤‘ë³µ ì œê±°
        df = df.dropDuplicates(df.columns)

        df.write.format("delta") \
            .option("mergeSchema", "true") \
            .mode("append") \
            .saveAsTable(catalog_table)

        print(f"âœ… Delta í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {len(data)}ê°œ ì €ì¥ë¨.")

# âœ… ì‹œê°„ëŒ€ ì„¤ì •
KST = timezone(timedelta(hours=9))

# âœ… API ìˆ˜ì§‘ í•¨ìˆ˜
def collect_data(key):
    page = 1
    print(f"\nğŸ“¦ '{key}' ì—…ì¢… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

    while True:
        params = {
            'serviceKey': "ì…ë ¥í•˜ì„¸ìš”",
            'type': 'json',
            'pageNo': page,
            'numOfRows': 1000,
            'divId': 'indsLclsCd',
            'key': key
        }

        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            items = response.json().get("body", {}).get("items", [])

            if not items:
                print(f"ğŸ“­ Page {page}: ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ.")
                break

            collected_time = datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')
            for item in items:
                item["collected_time"] = collected_time

            save_to_delta(items)
            print(f"ğŸ“„ Page {page}: {len(items)}ê°œ ì €ì¥ ì™„ë£Œ.")

            if len(items) < 1000:
                print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬.")
                break

            page += 1
            time.sleep(0.5)

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Page {page} ì˜¤ë¥˜ - {e}. ì¬ì‹œë„...")
            time.sleep(2)

# âœ… ë³‘ë ¬ ìˆ˜ì§‘ ì‹¤í–‰ í•¨ìˆ˜
def run_parallel_collection(keys, max_workers=5):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_key = {executor.submit(collect_data, key): key for key in keys}

        for future in as_completed(future_to_key):
            key = future_to_key[future]
            try:
                future.result()
                print(f"âœ… '{key}' ì—…ì¢… ìˆ˜ì§‘ ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ '{key}' ì—…ì¢… ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

# âœ… ì—…ì¢… ì½”ë“œ ë¦¬ìŠ¤íŠ¸
industry_keys = ["G2", "I1", "I2", "L1", "M1", "N1", "P1", "Q1", "R1", "S2"]

# âœ… ì‹¤í–‰
initialize_delta_table()
run_parallel_collection(industry_keys, max_workers=5)


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM  bronze.api_public.`ì—…ì¢…ë³„_ìƒê°€ì—…ì†Œ_ì¡°íšŒ`;

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import DoubleType

# Bronze í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë“œ
bronze_df = spark.table("bronze.api_public.`ì—…ì¢…ë³„_ìƒê°€ì—…ì†Œ_ì¡°íšŒ`")

# ì „ì²˜ë¦¬ ë° ìŠ¤í‚¤ë§ˆ ì •ë¦¬
silver_df = bronze_df \
    .dropDuplicates(["bizesId"]) \
    .withColumn("x", col("x").cast(DoubleType())) \
    .withColumn("y", col("y").cast(DoubleType())) \
    .withColumn("lon", col("lon").cast(DoubleType())) \
    .withColumn("lat", col("lat").cast(DoubleType())) \
    .withColumn("collected_time", to_timestamp("collected_time", "yyyy-MM-dd HH:mm:ss"))

# Silver í…Œì´ë¸”ë¡œ ì €ì¥ (ìŠ¤í‚¤ë§ˆ ìë™ ë³‘í•© í—ˆìš©)
silver_df.write \
    .format("delta") \
    .option("mergeSchema", "true") \
    .mode("overwrite") \
    .saveAsTable("silver.public.`ì—…ì¢…ë³„_ìƒê°€ì—…ì†Œ_ì¡°íšŒ`")

print("âœ… Silver Layer ì €ì¥ ì™„ë£Œ: 'silver.public.ì—…ì¢…ë³„_ìƒê°€ì—…ì†Œ_ì¡°íšŒ'")

